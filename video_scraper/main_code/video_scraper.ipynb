{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d066a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本项目主要用于测试对主项目图像爬取目标网站中视频数据的爬取功能.\n",
    "# 本地测试虚拟环境为：kf_bf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c01e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理第 1 页: https://c4afcd.com/video/kaifang/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6532\\1430313221.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"任务完成! 视频已保存到\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOUTPUT_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"视频列表已保存到\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCSV_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6532\\1430313221.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"正在处理第 {page} 页: {list_url}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m             \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6532\\1430313221.py\u001b[0m in \u001b[0;36mget_page\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# 添加随机延迟\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mdelay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mDELAY_BETWEEN_REQUESTS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mHEADERS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "这个版本的代码没有考虑代理服务器的问题。不适用代理，则原始网站可以访问，但视频下载可能会受到限制。\n",
    "在开启代理服务，而没有修改代码的情况下，可能会导致下载失败，报错如下：\n",
    "代码运行后出现如下错误：正在处理第 1 页: https://c4afcd.com/video/kaifang/\n",
    "请求失败: https://c4afcd.com/video/kaifang/, 错误: HTTPSConnectionPool(host='c4afcd.com', port=443): Max retries exceeded with url: /video/kaifang/ (Caused by ProxyError('Unable to connect to proxy', OSError(0, 'Error')))\n",
    "\n",
    "\n",
    "关闭代理服务器后，直接运行，则报错信息如下：\n",
    "正在处理第 1 页: https://c4afcd.com/video/kaifang/\n",
    "本页找到 28 个视频\n",
    "处理视频: 炮友激情操逼特写小嘴吃jb，小时激情不断操，骑乘暴插多毛骚穴，娇喘呻吟爽翻\n",
    "下载失败: https://d1.xia12345.com/video/202503/67dda0bdba8bde5fd5418e2b/hd.mp4, 错误: ('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None))\n",
    "\"\"\"\n",
    "import requests\n",
    "import base64\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 配置参数\n",
    "BASE_URL = \"https://c4afcd.com/video/kaifang/\"\n",
    "OUTPUT_DIR = \"../downloaded_videos\"\n",
    "CSV_FILE = \"./video_list.csv\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Referer\": BASE_URL\n",
    "}\n",
    "# 延迟设置 (单位: 秒)\n",
    "DELAY_BETWEEN_REQUESTS = (5, 10)  # 请求间的随机延迟范围\n",
    "DELAY_BETWEEN_PAGES = (6, 8)         # 页面间的随机延迟范围\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 解密函数 (模拟网页中的d函数)\n",
    "def decode_title(encoded_str):\n",
    "    try:\n",
    "        # Base64解码\n",
    "        decoded_bytes = base64.b64decode(encoded_str)\n",
    "        # 转换为字符串 (使用正确的编码，这里通常是utf-8)\n",
    "        decoded_str = decoded_bytes.decode('utf-8')\n",
    "        return decoded_str\n",
    "    except:\n",
    "        return \"解码失败\"\n",
    "\n",
    "# 获取页面内容\n",
    "def get_page(url):\n",
    "    try:\n",
    "        # 添加随机延迟\n",
    "        delay = random.uniform(*DELAY_BETWEEN_REQUESTS)\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        response = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        response.encoding = 'utf-8'\n",
    "        \n",
    "        # 检查响应状态\n",
    "        if response.status_code != 200:\n",
    "            print(f\"请求失败: {url}, 状态码: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"请求失败: {url}, 错误: {e}\")\n",
    "        return None\n",
    "\n",
    "# 解析列表页获取视频链接和标题\n",
    "def parse_list_page(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    videos = []\n",
    "    \n",
    "    for dd in soup.select('dd'):\n",
    "        a_tag = dd.find('a')\n",
    "        if not a_tag:\n",
    "            continue\n",
    "            \n",
    "        # 提取相对链接并转换为绝对URL\n",
    "        rel_url = a_tag.get('href')\n",
    "        if not rel_url:\n",
    "            continue\n",
    "        full_url = urljoin(BASE_URL, rel_url)\n",
    "        \n",
    "        # 提取加密标题\n",
    "        script_tag = a_tag.find('script')\n",
    "        if script_tag:\n",
    "            # 从script内容中提取加密字符串\n",
    "            script_text = script_tag.string\n",
    "            if script_text:\n",
    "                # 使用正则提取d('...')中的内容\n",
    "                match = re.search(r\"d\\('([^']+)'\\)\", script_text)\n",
    "                if match:\n",
    "                    encoded_title = match.group(1)\n",
    "                    title = decode_title(encoded_title)\n",
    "                else:\n",
    "                    # 如果无法提取加密标题，使用h3中的文本\n",
    "                    title = a_tag.find('h3').get_text(strip=True)\n",
    "            else:\n",
    "                title = a_tag.find('h3').get_text(strip=True)\n",
    "        else:\n",
    "            title = a_tag.find('h3').get_text(strip=True)\n",
    "        \n",
    "        # 清理标题中的非法字符\n",
    "        clean_title = re.sub(r'[\\\\/*?:\"<>|]', '', title).strip()\n",
    "        \n",
    "        if clean_title and full_url:\n",
    "            videos.append((clean_title, full_url))\n",
    "    \n",
    "    return videos\n",
    "\n",
    "# 解析详情页获取视频地址\n",
    "def parse_detail_page(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    download_div = soup.find('div', class_='download')\n",
    "    if download_div:\n",
    "        input_tag = download_div.find('input', id='url')\n",
    "        if input_tag:\n",
    "            return input_tag.get('value')\n",
    "    return None\n",
    "\n",
    "# 下载视频\n",
    "def download_video(url, filename):\n",
    "    safe_filename = re.sub(r'[\\\\/*?:\"<>|]', '', filename)\n",
    "    filepath = os.path.join(OUTPUT_DIR, f\"{safe_filename}.mp4\")\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"文件已存在: {safe_filename}\")\n",
    "        return filepath\n",
    "    \n",
    "    try:\n",
    "        # 添加随机延迟\n",
    "        delay = random.uniform(*DELAY_BETWEEN_REQUESTS)\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        with requests.get(url, stream=True, headers=HEADERS, timeout=60) as r:\n",
    "            r.raise_for_status()\n",
    "            \n",
    "            # 获取文件大小\n",
    "            total_size = int(r.headers.get('content-length', 0))\n",
    "            downloaded_size = 0\n",
    "            \n",
    "            with open(filepath, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    if chunk:  # 过滤掉保持连接的新块\n",
    "                        f.write(chunk)\n",
    "                        downloaded_size += len(chunk)\n",
    "                        # 显示进度\n",
    "                        if total_size > 0:\n",
    "                            percent = downloaded_size / total_size * 100\n",
    "                            print(f\"下载中: {safe_filename} - {percent:.1f}%\", end='\\r')\n",
    "            \n",
    "            print(f\"下载成功: {safe_filename} ({downloaded_size//1024}KB)\")\n",
    "            return filepath\n",
    "    except Exception as e:\n",
    "        print(f\"下载失败: {url}, 错误: {e}\")\n",
    "        return None\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 准备CSV文件\n",
    "    with open(CSV_FILE, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['视频文件名', '视频地址'])\n",
    "        \n",
    "        # 处理前10页\n",
    "        for page in range(1, 11):\n",
    "            if page == 1:\n",
    "                list_url = BASE_URL\n",
    "            else:\n",
    "                list_url = urljoin(BASE_URL, f\"index_{page}.html\")\n",
    "            \n",
    "            print(f\"正在处理第 {page} 页: {list_url}\")\n",
    "            html = get_page(list_url)\n",
    "            if not html:\n",
    "                continue\n",
    "                \n",
    "            videos = parse_list_page(html)\n",
    "            print(f\"本页找到 {len(videos)} 个视频\")\n",
    "            \n",
    "            for title, detail_url in videos:\n",
    "                print(f\"处理视频: {title}\")\n",
    "                \n",
    "                # 获取详情页\n",
    "                detail_html = get_page(detail_url)\n",
    "                if not detail_html:\n",
    "                    continue\n",
    "                    \n",
    "                # 提取视频地址\n",
    "                video_url = parse_detail_page(detail_html)\n",
    "                if not video_url:\n",
    "                    print(f\"未找到视频地址: {title}\")\n",
    "                    continue\n",
    "                \n",
    "                # 写入CSV\n",
    "                csv_writer.writerow([title, video_url])\n",
    "                csvfile.flush()  # 立即写入磁盘\n",
    "                \n",
    "                # 下载视频\n",
    "                download_video(video_url, title)\n",
    "            \n",
    "            # 页面间延迟\n",
    "            if page < 10:\n",
    "                delay = random.uniform(*DELAY_BETWEEN_PAGES)\n",
    "                print(f\"等待 {delay:.1f} 秒后继续下一页...\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    print(\"任务完成! 视频已保存到\", OUTPUT_DIR)\n",
    "    print(\"视频列表已保存到\", CSV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f41b94e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b4b029d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试网络连接: https://www.baidu.com\n",
      "网络连接测试成功! 状态码: 200\n",
      "============================================================\n",
      "使用系统代理设置\n",
      "============================================================\n",
      "检测到系统代理设置: {}\n",
      "检测到的系统代理: {}\n",
      "正在处理第 1 页: https://c4afcd.com/video/kaifang/\n",
      "请求: https://c4afcd.com/video/kaifang/\n",
      "检测到系统代理设置: {}\n",
      "本页找到 28 个视频\n",
      "处理视频 [1/28]: 炮友激情操逼特写小嘴吃jb，小时激情不断操，骑乘暴插多毛骚穴，娇喘呻吟爽翻\n",
      "请求: https://c4afcd.com/html/202506/111982.html\n",
      "检测到系统代理设置: {}\n",
      "文件已存在: 炮友激情操逼特写小嘴吃jb，小时激情不断操，骑乘暴插多毛骚穴，娇喘呻吟爽翻\n",
      "处理视频 [2/28]: 激情操逼特写小嘴吃jb，小时激情不断操，骑乘暴插多毛骚穴，娇喘呻吟爽翻\n",
      "请求: https://c4afcd.com/html/202506/111981.html\n",
      "检测到系统代理设置: {}\n",
      "文件已存在: 激情操逼特写小嘴吃jb，小时激情不断操，骑乘暴插多毛骚穴，娇喘呻吟爽翻\n",
      "处理视频 [3/28]: 激情啪啪，口活很棒让小哥吃奶舔逼舔菊花，各种体位无套抽插好刺激\n",
      "请求: https://c4afcd.com/html/202506/111977.html\n",
      "检测到系统代理设置: {}\n",
      "文件已存在: 激情啪啪，口活很棒让小哥吃奶舔逼舔菊花，各种体位无套抽插好刺激\n",
      "处理视频 [4/28]: 激情名场面，双屌轮流吃，唇钉嫩妹妹，穿上白丝骑乘位，自己上下动，侧入操嫩穴\n",
      "请求: https://c4afcd.com/html/202506/111922.html\n",
      "检测到系统代理设置: {}\n",
      "文件已存在: 激情名场面，双屌轮流吃，唇钉嫩妹妹，穿上白丝骑乘位，自己上下动，侧入操嫩穴\n",
      "处理视频 [5/28]: 漏奶情趣装，口交乳交大鸡巴，床上床下多体位蹂躏爆草，浪叫呻吟不止\n",
      "请求: https://c4afcd.com/html/202506/111921.html\n",
      "检测到系统代理设置: {}\n",
      "文件已存在: 漏奶情趣装，口交乳交大鸡巴，床上床下多体位蹂躏爆草，浪叫呻吟不止\n",
      "处理视频 [6/28]: 漂亮美眉身材娇小家具厂慰问工人小哥捉迷藏谁先找到有逼操否则就是看别人操逼的份了p原版\n",
      "请求: https://c4afcd.com/html/202506/111918.html\n",
      "检测到系统代理设置: {}\n",
      "文件已存在: 漂亮美眉身材娇小家具厂慰问工人小哥捉迷藏谁先找到有逼操否则就是看别人操逼的份了p原版\n",
      "处理视频 [7/28]: 漏奶情趣装，口交乳交大鸡巴，床上床下多体位蹂躏爆草，浪叫呻吟不止 ()\n",
      "请求: https://c4afcd.com/html/202506/111862.html\n",
      "检测到系统代理设置: {}\n",
      "文件已存在: 漏奶情趣装，口交乳交大鸡巴，床上床下多体位蹂躏爆草，浪叫呻吟不止 ()\n",
      "处理视频 [8/28]: 漂亮美女被大鸡吧无套爆菊花插的屁眼大开肠液流出鲍鱼一开一合最后口爆吃精高清p原版\n",
      "请求: https://c4afcd.com/html/202506/111860.html\n",
      "检测到系统代理设置: {}\n",
      "文件已存在: 漂亮美女被大鸡吧无套爆菊花插的屁眼大开肠液流出鲍鱼一开一合最后口爆吃精高清p原版\n",
      "处理视频 [9/28]: 漂亮大奶人妻吃鸡乳交身材丰满酒店约操大洋吊操遍房间每个角落被无套输出口爆射了满满一脸高清p原版\n",
      "请求: https://c4afcd.com/html/202506/111858.html\n",
      "检测到系统代理设置: {}\n",
      "文件已存在: 漂亮大奶人妻吃鸡乳交身材丰满酒店约操大洋吊操遍房间每个角落被无套输出口爆射了满满一脸高清p原版\n",
      "处理视频 [10/28]: 游泳找小虎，巨乳任你舔p原版\n",
      "请求: https://c4afcd.com/html/202506/111802.html\n",
      "检测到系统代理设置: {}\n",
      "文件已存在: 游泳找小虎，巨乳任你舔p原版\n",
      "处理视频 [11/28]: 湿滑无比汁液拉丝，操出白浆，啊爸爸操我，妖精吸阳采精 (4)\n",
      "请求: https://c4afcd.com/html/202506/111801.html\n",
      "检测到系统代理设置: {}\n",
      "文件已存在: 湿滑无比汁液拉丝，操出白浆，啊爸爸操我，妖精吸阳采精 (4)\n",
      "处理视频 [12/28]: 清纯诱惑完美结合，翘起小屁屁迎接大肉棒进入，你以为的女神背地里其实是个任人羞辱的小贱货\n",
      "请求: https://c4afcd.com/html/202506/111797.html\n",
      "检测到系统代理设置: {}\n",
      "文件已存在: 清纯诱惑完美结合，翘起小屁屁迎接大肉棒进入，你以为的女神背地里其实是个任人羞辱的小贱货\n",
      "处理视频 [13/28]: 清纯文静甜妹，被两老哥一起玩，按头猛插小嘴，一个插嘴一个扣穴，怼入小穴，太紧了，一顿爆草\n",
      "请求: https://c4afcd.com/html/202506/111742.html\n",
      "检测到系统代理设置: {}\n",
      "文件已存在: 清纯文静甜妹，被两老哥一起玩，按头猛插小嘴，一个插嘴一个扣穴，怼入小穴，太紧了，一顿爆草\n",
      "处理视频 [14/28]: 清纯小女友，出租屋操小骚逼，较小身材，跪着大屌插嘴，口活很棒舔蛋吸吮，骑乘位爆插，后入猛怼\n",
      "请求: https://c4afcd.com/html/202506/111741.html\n",
      "检测到系统代理设置: {}\n",
      "文件已存在: 清纯小女友，出租屋操小骚逼，较小身材，跪着大屌插嘴，口活很棒舔蛋吸吮，骑乘位爆插，后入猛怼\n",
      "处理视频 [15/28]: 清纯娃娃脸白虎美少女超可爱黑丝开档被主人中出，看似清纯少女床上真是骚，让我内射她的小嫩穴，妹妹超甜高清\n",
      "请求: https://c4afcd.com/html/202506/111737.html\n",
      "检测到系统代理设置: {}\n",
      "文件已存在: 清纯娃娃脸白虎美少女超可爱黑丝开档被主人中出，看似清纯少女床上真是骚，让我内射她的小嫩穴，妹妹超甜高清\n",
      "处理视频 [16/28]: 淫萝少女看av自慰房东追租正中下怀嫩穴肉偿大屌爆刺淫汁狂滴内射欲女\n",
      "请求: https://c4afcd.com/html/202506/111682.html\n",
      "检测到系统代理设置: {}\n",
      "开始下载: https://d1.xia12345.com/video/202503/67dda0bdba8bde5fd5418e1b/hd.mp4\n",
      "检测到系统代理设置: {}\n",
      "下载失败: https://d1.xia12345.com/video/202503/67dda0bdba8bde5fd5418e1b/hd.mp4, 错误: (\"Connection broken: ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)\", ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None))\n",
      "处理视频 [17/28]: 淫荡骚母狗男一女群p名场面酒店排着队吃吊，骑乘位边操边舔，后入前后夹击，轮流输出，搞的舒服了\n",
      "请求: https://c4afcd.com/html/202506/111681.html\n",
      "检测到系统代理设置: {}\n",
      "开始下载: https://d1.xia12345.com/video/202503/67dda0bdba8bde5fd5418e1a/hd.mp4\n",
      "检测到系统代理设置: {}\n",
      "下载中: 淫荡骚母狗男一女群p名场面酒店排着队吃吊，骑乘位边操边舔，后入前后夹击，轮流输出，搞的舒服了 - 100.0%\n",
      "下载成功: 淫荡骚母狗男一女群p名场面酒店排着队吃吊，骑乘位边操边舔，后入前后夹击，轮流输出，搞的舒服了 (188810KB)\n",
      "处理视频 [18/28]: 淫妇坐在发上调情结果被小伙狠草付费资源高清p原版\n",
      "请求: https://c4afcd.com/html/202506/111678.html\n",
      "检测到系统代理设置: {}\n",
      "开始下载: https://d1.xia12345.com/video/202503/67dda0bdba8bde5fd5418e18/hd.mp4\n",
      "检测到系统代理设置: {}\n",
      "下载中: 淫妇坐在发上调情结果被小伙狠草付费资源高清p原版 - 100.0%\n",
      "下载成功: 淫妇坐在发上调情结果被小伙狠草付费资源高清p原版 (83145KB)\n",
      "处理视频 [19/28]: 淫荡小母狗，可爱小学妹戴上小母狗尾巴肛塞，魔鬼身材超紧小嫩逼，床上喜欢一边被羞辱一边挨操又纯又\n",
      "请求: https://c4afcd.com/html/202506/111622.html\n",
      "检测到系统代理设置: {}\n",
      "开始下载: https://d1.xia12345.com/video/202503/67dda0bdba8bde5fd5418e19/hd.mp4\n",
      "检测到系统代理设置: {}\n",
      "下载失败: https://d1.xia12345.com/video/202503/67dda0bdba8bde5fd5418e19/hd.mp4, 错误: (\"Connection broken: ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)\", ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None))\n",
      "处理视频 [20/28]: 淫声荡语不断叫爸爸，小骚逼被好姐妹玩出好多淫水，嘴里吃几把穴被草\n",
      "请求: https://c4afcd.com/html/202506/111621.html\n",
      "检测到系统代理设置: {}\n",
      "开始下载: https://d1.xia12345.com/video/202503/67dda0bdba8bde5fd5418e17/hd.mp4\n",
      "检测到系统代理设置: {}\n",
      "下载失败: https://d1.xia12345.com/video/202503/67dda0bdba8bde5fd5418e17/hd.mp4, 错误: HTTPSConnectionPool(host='d1.xia12345.com', port=443): Max retries exceeded with url: /video/202503/67dda0bdba8bde5fd5418e17/hd.mp4 (Caused by ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)))\n",
      "处理视频 [21/28]: 泰国炮男的嫩萝玩物约炮小淫娃，淫声荡语，让小哥草了骚逼还草嘴，口爆吞精真刺激onlyfan\n",
      "请求: https://c4afcd.com/html/202506/111617.html\n",
      "检测到系统代理设置: {}\n",
      "开始下载: https://d1.xia12345.com/video/202503/67dda0bdba8bde5fd5418e15/hd.mp4\n",
      "检测到系统代理设置: {}\n",
      "下载失败: https://d1.xia12345.com/video/202503/67dda0bdba8bde5fd5418e15/hd.mp4, 错误: HTTPSConnectionPool(host='d1.xia12345.com', port=443): Max retries exceeded with url: /video/202503/67dda0bdba8bde5fd5418e15/hd.mp4 (Caused by ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)))\n",
      "处理视频 [22/28]: 淫交p群p淫乱派对各种啪啪露出调教，在老公眼前被单男调教轮着爆操完结高清p原版\n",
      "请求: https://c4afcd.com/html/202506/111562.html\n",
      "检测到系统代理设置: {}\n",
      "开始下载: https://d1.xia12345.com/video/202503/67dda0bdba8bde5fd5418e16/hd.mp4\n",
      "检测到系统代理设置: {}\n",
      "\n",
      "程序被用户中断\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import base64\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import urllib3\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "# 禁用安全警告\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# ======== 配置区域 ========\n",
    "# 基础URL\n",
    "BASE_URL = \"https://c4afcd.com/video/kaifang/\"\n",
    "# 输出目录\n",
    "OUTPUT_DIR = \"../downloaded_videos\"\n",
    "# CSV文件名\n",
    "CSV_FILE = \"video_list.csv\"\n",
    "# 使用系统代理设置\n",
    "USE_SYSTEM_PROXY = True  # 设置为True以使用系统代理设置\n",
    "# =========================\n",
    "\n",
    "# 请求头设置\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Referer\": BASE_URL,\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.9,en;q=0.8\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"DNT\": \"1\"\n",
    "}\n",
    "\n",
    "# 延迟设置 (单位: 秒)\n",
    "DELAY_BETWEEN_REQUESTS = (1.5, 3.0)\n",
    "DELAY_BETWEEN_PAGES = (3, 6)\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 创建带重试机制的会话\n",
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # 配置重试策略\n",
    "    retry_strategy = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=1,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\", \"POST\"]\n",
    "    )\n",
    "    \n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    session.mount(\"http://\", adapter)\n",
    "    session.mount(\"https://\", adapter)\n",
    "    \n",
    "    return session\n",
    "\n",
    "# 初始化会话\n",
    "session = create_session()\n",
    "\n",
    "# 获取系统代理设置\n",
    "def get_system_proxies():\n",
    "    try:\n",
    "        from urllib.request import getproxies\n",
    "        proxies = getproxies()\n",
    "        print(f\"检测到系统代理设置: {proxies}\")\n",
    "        return proxies\n",
    "    except:\n",
    "        print(\"未检测到系统代理设置\")\n",
    "        return {}\n",
    "\n",
    "# 解密函数 (模拟网页中的d函数)\n",
    "def decode_title(encoded_str):\n",
    "    try:\n",
    "        # Base64解码\n",
    "        decoded_bytes = base64.b64decode(encoded_str)\n",
    "        # 转换为字符串 (使用正确的编码，这里通常是utf-8)\n",
    "        decoded_str = decoded_bytes.decode('utf-8')\n",
    "        return decoded_str\n",
    "    except:\n",
    "        return \"解码失败\"\n",
    "\n",
    "# 获取页面内容\n",
    "def get_page(url):\n",
    "    try:\n",
    "        # 添加随机延迟\n",
    "        delay = random.uniform(*DELAY_BETWEEN_REQUESTS)\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        print(f\"请求: {url}\")\n",
    "        \n",
    "        # 根据设置选择是否使用代理\n",
    "        if USE_SYSTEM_PROXY:\n",
    "            proxies = get_system_proxies()\n",
    "        else:\n",
    "            proxies = {}\n",
    "        \n",
    "        response = session.get(\n",
    "            url, \n",
    "            headers=HEADERS, \n",
    "            timeout=15,\n",
    "            verify=False,  # 忽略SSL证书验证\n",
    "            proxies=proxies\n",
    "        )\n",
    "        \n",
    "        # 检查响应状态\n",
    "        if response.status_code != 200:\n",
    "            print(f\"请求失败: {url}, 状态码: {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        response.encoding = 'utf-8'\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"请求失败: {url}, 错误: {e}\")\n",
    "        return None\n",
    "\n",
    "# 解析列表页获取视频链接和标题\n",
    "def parse_list_page(html):\n",
    "    if not html:\n",
    "        return []\n",
    "        \n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        videos = []\n",
    "        \n",
    "        for dd in soup.select('dd'):\n",
    "            a_tag = dd.find('a')\n",
    "            if not a_tag:\n",
    "                continue\n",
    "                \n",
    "            # 提取相对链接并转换为绝对URL\n",
    "            rel_url = a_tag.get('href')\n",
    "            if not rel_url:\n",
    "                continue\n",
    "            full_url = urljoin(BASE_URL, rel_url)\n",
    "            \n",
    "            # 提取加密标题\n",
    "            script_tag = a_tag.find('script')\n",
    "            title = \"\"\n",
    "            if script_tag:\n",
    "                # 从script内容中提取加密字符串\n",
    "                script_text = script_tag.string\n",
    "                if script_text:\n",
    "                    # 使用正则提取d('...')中的内容\n",
    "                    match = re.search(r\"d\\('([^']+)'\\)\", script_text)\n",
    "                    if match:\n",
    "                        encoded_title = match.group(1)\n",
    "                        title = decode_title(encoded_title)\n",
    "                    else:\n",
    "                        # 如果无法提取加密标题，使用h3中的文本\n",
    "                        h3_tag = a_tag.find('h3')\n",
    "                        if h3_tag:\n",
    "                            title = h3_tag.get_text(strip=True)\n",
    "                else:\n",
    "                    h3_tag = a_tag.find('h3')\n",
    "                    if h3_tag:\n",
    "                        title = h3_tag.get_text(strip=True)\n",
    "            else:\n",
    "                h3_tag = a_tag.find('h3')\n",
    "                if h3_tag:\n",
    "                    title = h3_tag.get_text(strip=True)\n",
    "            \n",
    "            # 清理标题中的非法字符\n",
    "            clean_title = re.sub(r'[\\\\/*?:\"<>|]', '', title).strip()\n",
    "            \n",
    "            if clean_title and full_url:\n",
    "                videos.append((clean_title, full_url))\n",
    "        \n",
    "        return videos\n",
    "    except Exception as e:\n",
    "        print(f\"解析列表页失败: {e}\")\n",
    "        return []\n",
    "\n",
    "# 解析详情页获取视频地址\n",
    "def parse_detail_page(html):\n",
    "    if not html:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        download_div = soup.find('div', class_='download')\n",
    "        if download_div:\n",
    "            input_tag = download_div.find('input', id='url')\n",
    "            if input_tag:\n",
    "                return input_tag.get('value')\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"解析详情页失败: {e}\")\n",
    "        return None\n",
    "\n",
    "# 下载视频\n",
    "def download_video(url, filename):\n",
    "    safe_filename = re.sub(r'[\\\\/*?:\"<>|]', '', filename)\n",
    "    filepath = os.path.join(OUTPUT_DIR, f\"{safe_filename}.mp4\")\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        print(f\"文件已存在: {safe_filename}\")\n",
    "        return filepath\n",
    "    \n",
    "    try:\n",
    "        # 添加随机延迟\n",
    "        delay = random.uniform(*DELAY_BETWEEN_REQUESTS)\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        print(f\"开始下载: {url}\")\n",
    "        \n",
    "        # 根据设置选择是否使用代理\n",
    "        if USE_SYSTEM_PROXY:\n",
    "            proxies = get_system_proxies()\n",
    "        else:\n",
    "            proxies = {}\n",
    "        \n",
    "        with session.get(\n",
    "            url, \n",
    "            stream=True, \n",
    "            headers=HEADERS, \n",
    "            timeout=60,\n",
    "            verify=False,  # 忽略SSL证书验证\n",
    "            proxies=proxies\n",
    "        ) as r:\n",
    "            r.raise_for_status()\n",
    "            \n",
    "            # 获取文件大小\n",
    "            total_size = int(r.headers.get('content-length', 0))\n",
    "            downloaded_size = 0\n",
    "            \n",
    "            with open(filepath, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    if chunk:  # 过滤掉保持连接的新块\n",
    "                        f.write(chunk)\n",
    "                        downloaded_size += len(chunk)\n",
    "                        # 显示进度\n",
    "                        if total_size > 0:\n",
    "                            percent = downloaded_size / total_size * 100\n",
    "                            print(f\"下载中: {safe_filename} - {percent:.1f}%\", end='\\r')\n",
    "            \n",
    "            print(f\"\\n下载成功: {safe_filename} ({downloaded_size//1024}KB)\")\n",
    "            return filepath\n",
    "    except Exception as e:\n",
    "        print(f\"下载失败: {url}, 错误: {e}\")\n",
    "        return None\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 准备CSV文件\n",
    "    with open(CSV_FILE, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['视频文件名', '视频地址'])\n",
    "        \n",
    "        # 处理前10页\n",
    "        for page in range(1, 11):\n",
    "            if page == 1:\n",
    "                list_url = BASE_URL\n",
    "            else:\n",
    "                list_url = urljoin(BASE_URL, f\"index_{page}.html\")\n",
    "            \n",
    "            print(f\"正在处理第 {page} 页: {list_url}\")\n",
    "            html = get_page(list_url)\n",
    "            if not html:\n",
    "                print(f\"跳过第 {page} 页\")\n",
    "                continue\n",
    "                \n",
    "            videos = parse_list_page(html)\n",
    "            print(f\"本页找到 {len(videos)} 个视频\")\n",
    "            \n",
    "            for i, (title, detail_url) in enumerate(videos, 1):\n",
    "                print(f\"处理视频 [{i}/{len(videos)}]: {title}\")\n",
    "                \n",
    "                # 获取详情页\n",
    "                detail_html = get_page(detail_url)\n",
    "                if not detail_html:\n",
    "                    print(f\"跳过视频: {title}\")\n",
    "                    continue\n",
    "                    \n",
    "                # 提取视频地址\n",
    "                video_url = parse_detail_page(detail_html)\n",
    "                if not video_url:\n",
    "                    print(f\"未找到视频地址: {title}\")\n",
    "                    continue\n",
    "                \n",
    "                # 写入CSV\n",
    "                csv_writer.writerow([title, video_url])\n",
    "                csvfile.flush()  # 立即写入磁盘\n",
    "                \n",
    "                # 下载视频\n",
    "                download_video(video_url, title)\n",
    "            \n",
    "            # 页面间延迟\n",
    "            if page < 4:\n",
    "                delay = random.uniform(*DELAY_BETWEEN_PAGES)\n",
    "                print(f\"等待 {delay:.1f} 秒后继续下一页...\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # 测试网络连接\n",
    "        test_url = \"https://www.baidu.com\"\n",
    "        print(f\"测试网络连接: {test_url}\")\n",
    "        try:\n",
    "            response = session.get(test_url, timeout=5, verify=False)\n",
    "            print(f\"网络连接测试成功! 状态码: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"网络连接测试失败! 请检查网络设置: {e}\")\n",
    "            print(\"提示: 可能需要启用系统代理或VPN\")\n",
    "            exit(1)\n",
    "        \n",
    "        # 检查系统代理设置\n",
    "        if USE_SYSTEM_PROXY:\n",
    "            print(\"=\" * 60)\n",
    "            print(\"使用系统代理设置\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"检测到的系统代理: {get_system_proxies()}\")\n",
    "        else:\n",
    "            print(\"=\" * 60)\n",
    "            print(\"不使用代理\")\n",
    "            print(\"=\" * 60)\n",
    "        \n",
    "        main()\n",
    "        print(\"任务完成! 视频已保存到\", OUTPUT_DIR)\n",
    "        print(\"视频列表已保存到\", CSV_FILE)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n程序被用户中断\")\n",
    "    except Exception as e:\n",
    "        print(f\"程序运行出错: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kf_bf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
