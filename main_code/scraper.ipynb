{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ead0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本地运行环境为kf_bf\n",
    "\n",
    "# 数据文件已上传到google drive，地址见readme.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14837e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一个测试的爬虫脚本，爬取指定网站的图片信息并下载图片\n",
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 配置参数\n",
    "BASE_URL = \"https://a5c425.com/pic/toupai/\"\n",
    "OUTPUT_CSV = \"image_data.csv\"\n",
    "IMAGE_DIR = \"../downloaded_images\"\n",
    "PAGES = 100  # 要爬取的页数\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Referer': 'https://a5c425.com/'\n",
    "}\n",
    "\n",
    "# 创建图片保存目录\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "def decode_title(script_content):\n",
    "    \"\"\"从script标签中提取标题文本\"\"\"\n",
    "    # 查找标题文本的模式\n",
    "    pattern = r\"document\\.write\\(d\\('([^']+)'\\)\\)\"\n",
    "    match = re.search(pattern, script_content)\n",
    "    if match:\n",
    "        encoded_str = match.group(1)\n",
    "        # 这里应该使用网站实际的解密函数，但网站使用了自定义的d函数\n",
    "        # 由于无法获取d函数，我们直接返回编码后的字符串作为占位\n",
    "        return f\"[EncodedTitle:{encoded_str}]\"\n",
    "    return \"[NoTitle]\"\n",
    "\n",
    "def extract_image_info(page_content, page_url):\n",
    "    \"\"\"从页面内容中提取图片信息\"\"\"\n",
    "    soup = BeautifulSoup(page_content, 'lxml')\n",
    "    dl_tags = soup.find_all('dl')\n",
    "    \n",
    "    image_data = []\n",
    "    \n",
    "    for dl in dl_tags:\n",
    "        try:\n",
    "            # 提取图片URL\n",
    "            img_tag = dl.find('img', {'class': 'nature'})\n",
    "            if img_tag and 'data-original' in img_tag.attrs:\n",
    "                img_url = img_tag['data-original']\n",
    "                \n",
    "                # 提取标题\n",
    "                h3_tag = dl.find('h3')\n",
    "                if h3_tag:\n",
    "                    script_tag = h3_tag.find('script')\n",
    "                    if script_tag:\n",
    "                        title = decode_title(script_tag.string)\n",
    "                    else:\n",
    "                        # 直接提取</script>和</h3>之间的文本\n",
    "                        script_content = str(h3_tag)\n",
    "                        end_script = script_content.find('</script>')\n",
    "                        if end_script != -1:\n",
    "                            title_content = script_content[end_script + 9:]\n",
    "                            title = title_content.replace('</h3>', '').strip()\n",
    "                        else:\n",
    "                            title = h3_tag.get_text().strip()\n",
    "                else:\n",
    "                    title = \"Untitled\"\n",
    "                \n",
    "                image_data.append({\n",
    "                    'title': title,\n",
    "                    'url': img_url\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing element: {e}\")\n",
    "    \n",
    "    return image_data\n",
    "\n",
    "def download_image(url, save_path):\n",
    "    \"\"\"下载并保存图片\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            with open(save_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "    return False\n",
    "\n",
    "def scrape_website():\n",
    "    \"\"\"主爬虫函数\"\"\"\n",
    "    all_images = []\n",
    "    \n",
    "    # 准备CSV文件\n",
    "    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8-sig') as csvfile:\n",
    "        fieldnames = ['名称', '地址']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # 遍历每一页\n",
    "        for page in range(1, PAGES + 1):\n",
    "            if page == 1:\n",
    "                page_url = BASE_URL\n",
    "            else:\n",
    "                page_url = f\"{BASE_URL}index_{page}.html\"\n",
    "            \n",
    "            print(f\"Processing page {page}: {page_url}\")\n",
    "            \n",
    "            try:\n",
    "                # 获取页面内容\n",
    "                response = requests.get(page_url, headers=HEADERS, timeout=15)\n",
    "                response.encoding = 'utf-8'\n",
    "                \n",
    "                if response.status_code != 200:\n",
    "                    print(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n",
    "                    continue\n",
    "                \n",
    "                # 提取图片信息\n",
    "                page_images = extract_image_info(response.text, page_url)\n",
    "                \n",
    "                # 处理本页的每张图片\n",
    "                for idx, img_info in enumerate(page_images):\n",
    "                    # 写入CSV\n",
    "                    writer.writerow({\n",
    "                        '名称': img_info['title'],\n",
    "                        '地址': img_info['url']\n",
    "                    })\n",
    "                    \n",
    "                    # 下载图片\n",
    "                    img_ext = os.path.splitext(img_info['url'])[1] or '.jpg'\n",
    "                    img_name = f\"{page}_{idx}{img_ext}\"\n",
    "                    img_path = os.path.join(IMAGE_DIR, img_name)\n",
    "                    \n",
    "                    if download_image(img_info['url'], img_path):\n",
    "                        print(f\"  Downloaded: {img_name}\")\n",
    "                    else:\n",
    "                        print(f\"  Failed to download: {img_info['url']}\")\n",
    "                \n",
    "                all_images.extend(page_images)\n",
    "                print(f\"Found {len(page_images)} images on page {page}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing page {page}: {e}\")\n",
    "            \n",
    "            # 添加延迟避免被封\n",
    "            time.sleep(1.5)\n",
    "    \n",
    "    print(f\"\\nFinished! Total images: {len(all_images)}\")\n",
    "    print(f\"CSV file saved to: {OUTPUT_CSV}\")\n",
    "    print(f\"Images saved to: {IMAGE_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_website()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6451a27d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ad863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个和上面版本类似。就是将网站多个选项添加到了一个列表中，因为各个网页后续页面格式一致,都是添加了index_页码.html的形式。\n",
    "import os\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "# 配置参数 - 改为URL列表\n",
    "BASE_URLS = [\n",
    "    \"https://594b43.com/pic/meitui/\",\n",
    "    \"https://594b43.com/pic/oumei/\",  # 添加更多URL\n",
    "    \"https://594b43.com/pic/katong/\"\n",
    "]\n",
    "OUTPUT_CSV = \"image_data.csv\"\n",
    "IMAGE_DIR = \"../downloaded_images\"\n",
    "PAGES_PER_SITE = 100  # 每个网站爬取的页数\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Referer': 'https://a5c425.com/'\n",
    "}\n",
    "\n",
    "# 创建图片保存目录\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "def decode_title(script_content):\n",
    "    \"\"\"从script标签中提取标题文本\"\"\"\n",
    "    # 查找标题文本的模式\n",
    "    pattern = r\"document\\.write\\(d\\('([^']+)'\\)\\)\"\n",
    "    match = re.search(pattern, script_content)\n",
    "    if match:\n",
    "        encoded_str = match.group(1)\n",
    "        # 这里应该使用网站实际的解密函数\n",
    "        return f\"[EncodedTitle:{encoded_str}]\"\n",
    "    return \"[NoTitle]\"\n",
    "\n",
    "def extract_image_info(page_content, page_url):\n",
    "    \"\"\"从页面内容中提取图片信息\"\"\"\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    dl_tags = soup.find_all('dl')\n",
    "    \n",
    "    image_data = []\n",
    "    \n",
    "    for dl in dl_tags:\n",
    "        try:\n",
    "            # 提取图片URL\n",
    "            img_tag = dl.find('img', {'class': 'nature'})\n",
    "            if img_tag and 'data-original' in img_tag.attrs:\n",
    "                img_url = img_tag['data-original']\n",
    "                \n",
    "                # 提取标题\n",
    "                h3_tag = dl.find('h3')\n",
    "                if h3_tag:\n",
    "                    script_tag = h3_tag.find('script')\n",
    "                    if script_tag:\n",
    "                        title = decode_title(script_tag.string)\n",
    "                    else:\n",
    "                        # 直接提取</script>和</h3>之间的文本\n",
    "                        script_content = str(h3_tag)\n",
    "                        end_script = script_content.find('</script>')\n",
    "                        if end_script != -1:\n",
    "                            title_content = script_content[end_script + 9:]\n",
    "                            title = title_content.replace('</h3>', '').strip()\n",
    "                        else:\n",
    "                            title = h3_tag.get_text().strip()\n",
    "                else:\n",
    "                    title = \"Untitled\"\n",
    "                \n",
    "                # 清理文件名中的非法字符\n",
    "                safe_title = re.sub(r'[\\\\/*?:\"<>|]', '', title)[:100]\n",
    "                \n",
    "                image_data.append({\n",
    "                    'title': safe_title,\n",
    "                    'url': img_url\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing element: {e}\")\n",
    "    \n",
    "    return image_data\n",
    "\n",
    "def download_image(url, save_path):\n",
    "    \"\"\"下载并保存图片\"\"\"\n",
    "    try:\n",
    "        # 如果文件已存在则跳过\n",
    "        if os.path.exists(save_path):\n",
    "            print(f\"  Image already exists: {os.path.basename(save_path)}\")\n",
    "            return True\n",
    "            \n",
    "        response = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        if response.status_code == 200:\n",
    "            with open(save_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"  Failed to download: HTTP {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Failed to download {url}: {e}\")\n",
    "    return False\n",
    "\n",
    "def get_url_hash(url):\n",
    "    \"\"\"生成URL的短哈希用于文件名\"\"\"\n",
    "    return hashlib.md5(url.encode()).hexdigest()[:8]\n",
    "\n",
    "def scrape_website():\n",
    "    \"\"\"主爬虫函数\"\"\"\n",
    "    # 检查CSV文件是否存在以确定是否需要写入表头\n",
    "    write_header = not os.path.exists(OUTPUT_CSV)\n",
    "    \n",
    "    # 准备CSV文件(追加模式)\n",
    "    with open(OUTPUT_CSV, 'a', newline='', encoding='utf-8-sig') as csvfile:\n",
    "        fieldnames = ['名称', '地址']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        # 只在第一次运行时写入表头\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        # 遍历所有BASE_URL\n",
    "        for url_idx, base_url in enumerate(BASE_URLS):\n",
    "            print(f\"\\n{'='*40}\")\n",
    "            print(f\"Processing site #{url_idx+1}: {base_url}\")\n",
    "            print(f\"{'='*40}\")\n",
    "            \n",
    "            # 遍历每一页\n",
    "            for page in range(1, PAGES_PER_SITE + 1):\n",
    "                if page == 1:\n",
    "                    page_url = base_url\n",
    "                else:\n",
    "                    page_url = f\"{base_url}index_{page}.html\"\n",
    "                \n",
    "                print(f\"\\nProcessing page {page}: {page_url}\")\n",
    "                \n",
    "                try:\n",
    "                    # 获取页面内容\n",
    "                    response = requests.get(page_url, headers=HEADERS, timeout=20)\n",
    "                    response.encoding = 'utf-8'\n",
    "                    \n",
    "                    if response.status_code != 200:\n",
    "                        print(f\"  Failed to fetch page. Status code: {response.status_code}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # 提取图片信息\n",
    "                    page_images = extract_image_info(response.text, page_url)\n",
    "                    \n",
    "                    # 处理本页的每张图片\n",
    "                    for idx, img_info in enumerate(page_images):\n",
    "                        # 写入CSV\n",
    "                        writer.writerow({\n",
    "                            '名称': img_info['title'],\n",
    "                            '地址': img_info['url']\n",
    "                        })\n",
    "                        \n",
    "                        # 生成唯一文件名\n",
    "                        url_hash = get_url_hash(img_info['url'])\n",
    "                        img_ext = os.path.splitext(img_info['url'])[1][:5]  # 获取扩展名\n",
    "                        if not img_ext or len(img_ext) > 5:\n",
    "                            img_ext = '.jpg'\n",
    "                            \n",
    "                        img_name = f\"site{url_idx}_page{page}_{idx}_{url_hash}{img_ext}\"\n",
    "                        img_path = os.path.join(IMAGE_DIR, img_name)\n",
    "                        \n",
    "                        # 下载图片\n",
    "                        if download_image(img_info['url'], img_path):\n",
    "                            print(f\"  ✓ Downloaded: {img_name}\")\n",
    "                        else:\n",
    "                            print(f\"  ✕ Failed to download: {img_info['url']}\")\n",
    "                    \n",
    "                    print(f\"  Found {len(page_images)} images on page {page}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error processing page: {str(e)[:100]}\")\n",
    "                \n",
    "                # 添加延迟避免被封\n",
    "                time.sleep(2.0)\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(\"Finished! All sites processed.\")\n",
    "    print(f\"CSV file: {OUTPUT_CSV}\")\n",
    "    print(f\"Images folder: {IMAGE_DIR}\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_website()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kf_bf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
